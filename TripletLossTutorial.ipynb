{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41645725",
   "metadata": {},
   "source": [
    "# Tutorial: TripletLoss for re-identification using FastAI v2\n",
    "\n",
    "This tutorial demonstrates the use of the `TripleLoss` loss function in FastAI v2 (running over Pytorch). TripletLoss is one of the leading ways to match an image (or sound or other signal) with a large database of images, even if there are very few matches for that particular input. Joint-embedding networks (aka Twin or Siamese networks) are a common alternative for this \"low k-shot\" challenge. According to [that article I read](tk), triplet loss outperformed twin networks in all of the tk studies where both approaches were tried. Another option for re-identification is Scale Invariant Feature Transform (SIFT) which is implemented in OpenCV. I haven't seen any papers comparing SIFT and statistical ML techniques in real-world tasks. \n",
    "\n",
    "The tutorial walks you through:\n",
    "\n",
    "- Creating the necessary conda environment\n",
    "- Downloading and expanding the MNIST dataset of handwritten digits\n",
    "- Creating and training a model using triplet loss\n",
    "- Applying this model to a few images of a new glyph (a hand-drawn star)\n",
    "- Finding the closest neighbors to the new images\n",
    "- Visualizing that the model tightly clusters the new glyph\n",
    "- Demonstrating that the model can be used for re-identification of the new glyph\n",
    "\n",
    "MNIST is an easy problem for modern statistical ML, with relatively small data and a small input size. This makes it a good candidate for a tutorial. I will try to highlight the places where you'll need to alter parameters when faced with a more realistic task, like animal re-identification. \n",
    "\n",
    "The code assumes that you have a CUDA-accelerated GPU for training and inferencing. The tutorial runs quickly on a 2080 (about 30 seconds per epoch during training, about 20ms to inference) but runs _much_ slower on a mobile GPU like a GeForce GTX 1650 (30 minutes per epoch on my Surface Book 3). I assume that porting it to CPU wouldn't be difficult, but I think it would be quite slow. If you don't have a local GPU, I'd suggest using a GPU-powered cloud compute resource, such as Azure ML. (Let me know if you'd like to see a tutorial on running this in Azure ML.) \n",
    "\n",
    "## The problem of re-identification\n",
    "\n",
    "Our photo libraries are filled with images of our friends and family. A machine learning model that focuses on \"classification\" can successfully tell us that we have lots of photos of \"smiling man\" or \"smiling woman.\" Such models will not, generally, tell us which photos are of Uncle Al and Aunt Betty. _That_ task -- re-identifying individuals -- is a different problem and requires different approaches.\n",
    "\n",
    "Beyond identifying Uncle Al and Aunt Betty in a photo library, re-identification is a common problem for wildlife biologists. Many species have photographic catalogs taken over years and decades, and many species have some distinctive features that can be used to identify individuals. \n",
    "\n",
    "### Some animals and how they may be re-identified\n",
    "\n",
    "| Species | &nbsp | &nbsp |\n",
    "| --- | --- | --- |\n",
    "| Humpback Whales | tk | tk |\n",
    "| Tigers | tk | tk | \n",
    "| Dolphins | tk | tk | \n",
    "| Manta Rays | tk | tk | \n",
    "\n",
    "\n",
    "## Create the Python environment \n",
    "\n",
    "I developed this tutorial on Ubuntu 20.04, Cuda 10.2, PyTorch 1.7, FastAI 2.3. I have to admit that the rigmarole of exactly recreating a GPU-enabled virtual environment is a little beyond me, but I _think_ you have to install CUDA manually and then when you run the `conda create` command below, I _think_ it will download properly-configured versions of the various libraries. \n",
    "\n",
    "Prequisites: \n",
    "1. [Install conda](tk) \n",
    "1. [Install CUDA](tk)\n",
    "\n",
    "1. Clone this repo and change into the directory.\n",
    "1. Create the environment:\n",
    "    ```bash\n",
    "    conda create -f 'conda_environment.yml'\n",
    "    ```\n",
    "1. Activate the environment for use with:\n",
    "   ```bash\n",
    "   conda activate fastai\n",
    "   ```\n",
    "1. Confirm the environment by running:\n",
    "   ```bash\n",
    "   python .\\versions.py\n",
    "   ```\n",
    "Which should result in something similar to:\n",
    "```bash\n",
    "Pytorch : 1.7.0, FastAI : 2.3.1, CUDA? : True\n",
    "```\n",
    "\n",
    "The environment also installs Jupyter. You may need to restart your terminal session to have the `jupyter` in your path. Run:\n",
    "\n",
    "```bash\n",
    "jupyter notebook TripletLossTutorial.ipynb\n",
    "```\n",
    "\n",
    "and continue from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d736f",
   "metadata": {},
   "source": [
    "# Tutorial: TripletLoss for re-identification using FastAI v2\n",
    "\n",
    "**If you have not done so, please see [README.md](readme.md) for instructions on creating the Python environment for this tutorial.**\n",
    "\n",
    "## Import packages\n",
    "\n",
    "Nothing surprising here, I think. To visualize the output, we're going to use scikit-learn's TSNE implementation. I've heard this isn't the fastest TSNE, but for 10K datapoints and a limited number of dimensions, it's fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70695725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.basics import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.learner import cnn_learner\n",
    "from fastai.vision.models import resnet34\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.data.core import DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf0f774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from loss_functions.triplet_loss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba5fc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eee8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5612f",
   "metadata": {},
   "source": [
    "## Confirm versions and CUDA\n",
    "\n",
    "This is just to confirm you're running GPU-accelerated. If you don't have a GPU, I think training will be very slow. (Let me know if I'm wrong!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6db7e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f72706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6a0b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3875",
   "metadata": {},
   "source": [
    "## Setup the training data\n",
    "\n",
    "The training data for this tutorial is the MNIST dataset of hand-written digits 0-9. The dataset contains 10K PNG images of size 28x28. The PNGs are 3-channel, but all the pixels are grayscale. Dear ol' MNIST.\n",
    "\n",
    "![A few MNIST digits](./media/mnist_sample.png)\n",
    "\n",
    "Now, you're probably used to MNIST as a dataset for _image classification_: \"To which of the trained-on categories ('0'...'9') does this query image most likely belong?\" But for this tutorial, we're using it to learn \"glyph re-identification.\" Our acid test will be seeing if images of a glyph not seen during training (a star) can be re-identified with very few samples (aka \"low shot\").\n",
    "\n",
    "`URLs.MNIST`, `untar_data()`, and `get_image_files()` are from the `fastai.basics` module. The following cell will download, if necessary, the MNIST dataset and unpack it. By default, the data will be stored in `~/.fastai/data`. (It's a good idea to keep track of the size of that directory when fooling around with FastAI! Some of the datasets are _big_!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0963c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = untar_data(URLs.MNIST)\n",
    "fnames = get_image_files(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112b04d",
   "metadata": {},
   "source": [
    "Now, `~/.fastai/data/mnist_ping/` contains `testing/` and `training/` directories, which in turn have `0/`, `1/`, `2/`, etc. subdirectories. FastAI v2's `ImageDataLoaders` class can process this typical directory structure and take care of the boilerplate of testing vs. training sets, loading and transforming the images appropriately for processing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc11dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper label for an image is the name of the directory it's in. e.g., \"1\" is the proper label for `1/1.png`\n",
    "def label_func(x): return x.parent.name\n",
    "\n",
    "dls = ImageDataLoaders.from_path_func(mnist, fnames, label_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3474134",
   "metadata": {},
   "source": [
    "The output of a TripletLoss-based model is an embedding (a vector representing a point in a high-dimensional space). The goal is that each of the dimensions in this embedding space represents a feature in the \"how to tell individuals of this class apart\" solution space. The distance between any two embeddings \n",
    "\n",
    "After training, the model generates embedding values for a query image. \n",
    "\n",
    "In a real-world animal re-identification model, this might be on the order of 128. In the case of MNIST, we can get fine results with just a few output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_embedding_length = 128\n",
    "output_embedding_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e56f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.valid_dl.new(shuffle=True)\n",
    "# Original is a fastai data (valid_dl etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2448f",
   "metadata": {},
   "source": [
    "To create a TripletLoss-based embedding, we'll start with a standard Convolutional Neural Network using our recently created `dls` `ImageDataLoaders` object, the ResNet34 architecture, and FastAI's (PyTorch's?) TripletLoss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet34, metrics=accuracy, loss_func=TripletLoss(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ba4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a1209",
   "metadata": {},
   "source": [
    "We want to replace the classification head of the ResNet34 architecture with an embedding output.  The `L2_norm` class normalizes the values to the range $ (0.0,1.0) $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab61b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2_norm, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = learn.model[1]\n",
    "learn.model[1] = nn.Sequential(layers[0], layers[1], layers[2], layers[3], nn.Linear(in_features=1024, out_features=output_embedding_length, bias=False), L2_norm()).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c99bc",
   "metadata": {},
   "source": [
    "We now have a model configured for TripletLoss. What happens behind the scene when this model is trained? \n",
    "\n",
    "Basically, the training loop will grab 3 images: a `target` image, a `positive` image, and a `negative` image. The `target` and `positive` are in the same class ('1', '2', '3', etc.) while the `negative` image is in a different class. The basic goal of training is to find weights such that `target` and `positive` have embedding values that are near other while `target` and `negative` have embedding values that are far from each other. \n",
    "\n",
    "If you think about that, _most_ triplets are going to end up being 'easy'; any two random '9's are likely to share several features, while a randomly selected not-9 is unlikely to be \"oh, yeah, that could be a 9.\" So training with just-random triplets is less efficient than training with 'hard' triplets. In a hard triplet, the `positive` and `negative` are as close as possible to the dividing line in \"tell glyphs apart\" embedding space. In an ideal 'hard' triplet with a `target` in class '9', the 'positive' is a '9' image that's close in appearance to a '7' image that could be mistaken for a '9'. Finding hard triplets and using them for training is somewhere between \"valuable\" and \"essential\" in real-world re-identification tasks. \"Triplet mining\" is outside the scope of this tutorial, but you can read more about it [here](tk). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b71d59",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "FastAI's `lr_find()` function does a quick sweep of learning rates. Typically, a good learning rate is one where the slope of the `lr_find()` graph is steepest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389dfac7",
   "metadata": {},
   "source": [
    "For tutorial purposes and because MNIST is pretty easy, we're going to use a fast learning rate and few epochs. Real-world data sets won't be so kind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ffae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "epochs = 20\n",
    "learn.fit_one_cycle(epochs, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{epochs}_epochs_{output_embedding_length}_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a547b24",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "To get the output embedding of a `query` image, we just use `learn.predict()` on our trained `Learner: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = PILImage.create('stars/Star1.png')\n",
    "result = learn.predict(query_img)\n",
    "query_fingerprint = result[1].numpy()\n",
    "print(query_fingerprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3680b4",
   "metadata": {},
   "source": [
    "### Fingerprinting the exemplars\n",
    "\n",
    "Create the database to which we will compare `query_fingerprint`:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(Path(mnist/'testing').rglob('*.png'))\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8fb50",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Outputs a blank line for each for some reason. Use %%capture when calling\n",
    "def fingerprint_all(fnames):\n",
    "    fingerprints = {}\n",
    "    for f in fnames:\n",
    "        category = label_func(f)\n",
    "        img = PILImage.create(f)\n",
    "        result = learn.predict(img)\n",
    "        fingerprint = result[1].numpy()\n",
    "        fingerprints[(category,f)] = fingerprint\n",
    "    return fingerprints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f462a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Suppresses output\n",
    "# Takes about 3 minutes on a 2080\n",
    "fingerprint_db = fingerprint_all(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9cdbf",
   "metadata": {},
   "source": [
    "## Nearest k-neighbors\n",
    "\n",
    "Because our embedding values are normalized, finding the nearest k-neighbors is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find k nearest neighbour using cosine similarity. Normalized vectors, so easy...\n",
    "def find_k_nearest_neighbors(vectors,vec,k):\n",
    "    dist_arr = np.matmul(vectors, vec.T)\n",
    "    return np.argsort(-dist_arr.flatten())[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = list(fingerprint_db.values())\n",
    "closest = find_k_nearest_neighbors(fps, query_fingerprint, 10)\n",
    "closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f64fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(fingerprint_db.items())[closest[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09966276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Put it together\n",
    "\n",
    "def best_match_mnist(fingerprint_db, fingerprint, k):\n",
    "    fps = list(fingerprint_db.values())\n",
    "    keys = list(fingerprint_db.keys())\n",
    "    match_indices = find_k_nearest_neighbors(fps,fingerprint,k)\n",
    "    for i in match_indices:\n",
    "        match_fp = fps[i]\n",
    "        distance = spatial.distance.cosine(match_fp, fingerprint)\n",
    "        yield (keys[i], distance)\n",
    "        \n",
    "list(best_match_mnist(fingerprint_db, query_fingerprint, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c0e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fingerprint_file(path) :\n",
    "    img = PILImage.create(path)\n",
    "    result = learn.predict(img)\n",
    "    fingerprint = result[1].numpy()\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2342ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp4 = fingerprint_file('media/4.png')\n",
    "list(best_match_mnist(fingerprint_db, fp4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps1 = fingerprint_file('stars/Star1.png')\n",
    "fps2 = fingerprint_file('stars/Star2.png')\n",
    "list(best_match_mnist(fingerprint_db, fps1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial.distance.cosine(fps2, fps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(best_match_mnist(fingerprint_db, fps2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_db[('*','/stars/Star1.png')] = fps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc200ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(list(m for m in best_match_mnist(fingerprint_db, fps2, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9407f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(list(m for m in best_match_mnist(fingerprint_db, fps2, 10000) if m[0][0] != \"8\"))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab962be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = PILImage.create(\"stars/Star2.png\")\n",
    "query_img"
   ]
  },
  {
   "cell_type": "raw",
   "id": "883a839c",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(4., 4.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(3, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "imgs = (PILImage.create(m[0][1]) for m in best_match_mnist(fingerprint_db, fps2, 9))\n",
    "\n",
    "for ax, im in zip(grid, imgs):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.create(\"stars/Star1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878c7d",
   "metadata": {},
   "source": [
    "I think this net is overfitted to numbers. But does that make sense? Shouldn't just... features to features create the appropriate cosine distance between star and star?\n",
    "\n",
    "Or, maybe it's that the best diagnostic is: \"OK, return the top 3 returns in the top 3 categories.\" So you'd get \"8\"s and their <0.04 distance, and you'd get FPS1 at 0.04, and then you'd get the top (whatever) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505af5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = np.array(list(fingerprint_db.values()))\n",
    "fps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90157da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_fingerprint(epochs, learning_rate, files_to_fingerprint) :\n",
    "    learn.fit_one_cycle(epochs,slice(learning_rate))\n",
    "    fingerprints = fingerprint_all(files_to_fingerprint)\n",
    "    \n",
    "    return (learn, fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7928da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(fingerprints_db, fingerprints_to_highlight = [], indices_to_highlight = []):\n",
    "    l = list(fingerprint_db.values())\n",
    "    fps = np.array(l + fingerprints_to_highlight)\n",
    "    tsne = TSNE(2, random_state = 42, verbose = 1)    \n",
    "    tsne_proj = tsne.fit_transform(fps)\n",
    "    \n",
    "    cmap = cm.get_cmap('tab20')\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    num_categories = 11\n",
    "\n",
    "    for i in range(num_categories):\n",
    "        matching_indices = [key for key, (val,_) in enumerate(fingerprint_db.keys()) if val == str(i)]\n",
    "        plt.scatter(tsne_proj[matching_indices,0], tsne_proj[matching_indices,1], c = np.array(cmap(i)).reshape(1,4), label = i)\n",
    "    ax.legend(fontsize='large', markerscale=2)\n",
    "    \n",
    "    # Highlight\n",
    "    plt.scatter(tsne_proj[indices_to_highlight,0], tsne_proj[indices_to_highlight,1], marker = '+', c = 'k', label = 'highlight') \n",
    "    ixs_to_highlight = range(len(fps) - len(fingerprints_to_highlight),len(fps))\n",
    "    (xs, ys) = (tsne_proj[ixs_to_highlight,0], tsne_proj[ixs_to_highlight,1])\n",
    "    plt.scatter(xs, ys, marker = '+', c = 'k', label = 'highlight') \n",
    "    (min_x, min_y, max_x, max_y) = (min(xs), min(ys), max(xs), max(ys))\n",
    "    min_x = min_x - 2\n",
    "    min_y = min_y - 2\n",
    "    width = max_x - min_x + 2\n",
    "    height = max_y - min_y + 2\n",
    "    rect = patches.Rectangle((min_x, min_y), width, height, linewidth=1, edgecolor='k', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    plt.savefig(model_name)\n",
    "    plt.show()\n",
    "    return tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Use %%capture to suppress fingerprint_all output, which produces a blank line for each file for some reason\n",
    "(_, fps) = train_and_fingerprint(0, .0005, fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d031282",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(fps, fingerprints_to_highlight = [fps1, fps2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a137789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
